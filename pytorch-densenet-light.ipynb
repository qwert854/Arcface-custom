{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import os\n",
    "import torchvision.datasets as dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터셋 및 모델저장 경로 지정\n",
    "dataset_path = \"/home/ycs/Downloads/dataset\"\n",
    "model_weight_save_path = \"/home/ycs/Downloads/save/\"\n",
    "num_classes = 400\n",
    "\n",
    "batch_size = 128\n",
    "num_workers = 3\n",
    "learning_rate = 0.0001\n",
    "layers = 100\n",
    "epoch = 100\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 및 테스트 데이터셋 경로 로드\n",
    "traindir = os.path.join(dataset_path, '/home/ycs/Downloads/dataset/train')\n",
    "testdir = os.path.join(dataset_path, '/home/ycs/Downloads/dataset/train')\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.ToTensor(),\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(testdir, transforms.Compose([\n",
    "        transforms.Resize(32),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate = 0.0):\n",
    "        #input dimsnsion을 정하고, output dimension을 정하고(growh_rate임), dropRate를 정함.\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True) # inplace 하면 input으로 들어온 것 자체를 수정하겠다는 뜻. 메모리 usage가 좀 좋아짐. 하지만 input을 없앰.\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride = 1, padding = 1, bias = False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout (out,p=self.droprate,training = self.training)\n",
    "        return torch.cat([x,out],1)\n",
    "        \n",
    "class BottleneckBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
    "        #out_planes => growh_rate를 입력으로 받게 된다.\n",
    "        super(BottleneckBlock,self).__init__()\n",
    "        inter_planes = out_planes * 4 # bottleneck layer의 conv 1x1 filter chennel 수는 4*growh_rate이다.\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        self.conv1 = nn.Conv2d(in_planes,inter_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(inter_planes)\n",
    "        self.conv2 = nn.Conv2d(inter_planes,out_planes,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
    "        out = self.conv2(self.relu(self.bn2(out)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training = self.training)\n",
    "        return torch.cat([x,out],1) # 입력으로 받은 x와 새로 만든 output을 합쳐서 내보낸다\n",
    "    \n",
    "class DenseBlock(nn.Module):\n",
    "    def __init__(self,nb_layers,in_planes,growh_rate,block,dropRate=0.0):\n",
    "        super(DenseBlock,self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, growh_rate, nb_layers, dropRate)\n",
    "    \n",
    "    def _make_layer(self,block,in_planes,growh_rate,nb_layers,dropRate):\n",
    "        layers=[]\n",
    "        for i in range(nb_layers):\n",
    "            layers.append(block(in_planes + i*growh_rate ,growh_rate,dropRate))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.layer(x)\n",
    "    \n",
    "class TransitionBlock(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes,dropRate=0.0):\n",
    "        super(TransitionBlock,self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.droprate = dropRate\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(self.relu(self.bn1(x)))\n",
    "        if self.droprate>0:\n",
    "            out = F.dropout(out,p=self.droprate,inplace=False,training=self.training)\n",
    "        return F.avg_pool2d(out,2)\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,depth,num_classes,growh_rate=12,reduction=0.5,bottleneck=True,dropRate=0.0):\n",
    "        super(DenseNet,self).__init__()\n",
    "        num_of_blocks = 3\n",
    "        in_planes = 16 # 2 * growh_rate\n",
    "        n = (depth - num_of_blocks - 1)/num_of_blocks # 총 depth에서 첫 conv , 2개의 transit , 마지막 linear 빼고 / num_of_blocks\n",
    "        if reduction != 1 :\n",
    "            in_planes = 2 * growh_rate\n",
    "        if bottleneck == True:\n",
    "            in_planes = 2 * growh_rate #논문에서 Bottleneck + Compression 할 경우 first layer은 2*growh_rate라고 했다.\n",
    "            n = n/2 # conv 1x1 레이어가 추가되니까 !\n",
    "            block = BottleneckBlock \n",
    "        else :\n",
    "            block = BasicBlock\n",
    "        \n",
    "        n = int(n) #n = DenseBlock에서 block layer 개수를 의미한다.\n",
    "        self.conv1 = nn.Conv2d(3,in_planes,kernel_size=3,stride=1,padding=1,bias=False) # input:RGB -> output:growhR*2\n",
    "        \n",
    "        \n",
    "        #1st block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block1 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans1 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        \n",
    "        \n",
    "        #2nd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block2 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        # in_planes,out_planes,dropRate\n",
    "        self.trans2 = TransitionBlock(in_planes, int(math.floor(in_planes*reduction)),dropRate=dropRate)\n",
    "        in_planes = int(math.floor(in_planes*reduction))\n",
    "        \n",
    "        \n",
    "        #3rd block\n",
    "        # nb_layers,in_planes,growh_rate,block,dropRate\n",
    "        self.block3 = DenseBlock(n,in_planes,growh_rate,block,dropRate)\n",
    "        in_planes = int(in_planes+n*growh_rate) # 입력 + 레이어 만큼의 growh_rate\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace = True)\n",
    "        \n",
    "        self.fc = nn.Linear(in_planes,num_classes) # 마지막에 ave_pool 후에 1x1 size의 결과만 남음.\n",
    "        \n",
    "        self.in_planes = in_planes\n",
    "        \n",
    "        # module 초기화\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                # Conv layer들은 필터에서 나오는 분산 root(2/n)로 normalize 함\n",
    "                # mean = 0 , 분산 = sqrt(2/n) // 이게 무슨 초기화 방법이었는지 기억이 안난다.\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d): # shifting param이랑 scaling param 초기화(?)\n",
    "                m.weight.data.fill_(1) # \n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):# linear layer 초기화.\n",
    "                m.bias.data.zero_()\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #x : 32*32\n",
    "        out = self.conv1(x) # 32*32\n",
    "        out = self.block1(out) # 32*32\n",
    "        out = self.trans1(out) # 16*16\n",
    "        out = self.block2(out) # 16*16\n",
    "        out = self.trans2(out) # 8*8\n",
    "        out = self.block3(out) # 8*8\n",
    "        out = self.relu(self.bn1(out)) #8*8\n",
    "        out = F.avg_pool2d(out,8) #1*1\n",
    "        out = out.view(-1, self.in_planes) #channel수만 남기 때문에 Linear -> in_planes\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DenseNet(layers,400,growh_rate=12,dropRate = 0.0) #400=클래스수 위에서 입력한 값과 동일하게 바꿔줘야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(),lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader,model,criterion,optimizer,epoch):\n",
    "    model.train()\n",
    "    for i, (input,target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(i%10 == 0): #10 step 마다 loss값 출력\n",
    "            print(\"loss in epoch %d , step %d : %f\" % (epoch, i,loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader,model,criterion,epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    correct = 0\n",
    "    \n",
    "    \n",
    "    for i, (input,target) in enumerate(test_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "        \n",
    "        output = model(input)\n",
    "        loss = criterion(output,target)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().float().sum()\n",
    "    \n",
    "    print(\"Accuracy in epoch %d : %f\" % (epoch,100.0*correct/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_lr(optimizer, epoch, learning_rate): #epoch 10 증가시 lr 값 변화 (변화없음=1)\n",
    "    if epoch==10 :\n",
    "        learning_rate*=1\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(0,100):\n",
    "    adjust_lr(optimizer,epoch,learning_rate)\n",
    "    train(train_loader,model,criterion,optimizer,epoch)\n",
    "    test(test_loader,model,criterion,epoch)\n",
    "    if epoch % 1 == 0: #1회당 한번저장\n",
    "        torch.save(model.state_dict(), model_weight_save_path + 'model.pt', _use_new_zipfile_serialization=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
