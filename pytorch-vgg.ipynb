{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    " \n",
    "\n",
    "class VggFace(torch.nn.Module):\n",
    "    def __init__(self, classes=2622):\n",
    "        \"\"\"VGGFace model.\n",
    "        Face recognition network.  It takes as input a Bx3x224x224\n",
    "        batch of face images and gives as output a BxC score vector\n",
    "        (C is the number of identities).\n",
    "        Input images need to be scaled in the 0-1 range and then \n",
    "        normalized with respect to the mean RGB used during training.\n",
    "        Args:\n",
    "            classes (int): number of identities recognized by the\n",
    "            network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = _ConvBlock(3, 32, 32)\n",
    "        self.conv2 = _ConvBlock(32, 64, 64)\n",
    "        self.conv3 = _ConvBlock(64, 128, 128, 128)\n",
    "        self.conv4 = _ConvBlock(128, 256, 256, 256)\n",
    "        self.conv5 = _ConvBlock(256, 512, 512, 512)\n",
    "        self.dropout = torch.nn.Dropout(0.5)\n",
    "        self.fc1 = torch.nn.Linear(7 * 7 * 512, 4096)\n",
    "        self.fc2 = torch.nn.Linear(4096, 4096)\n",
    "        self.fc3 = torch.nn.Linear(4096, classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        x = self.dropout(F.relu(self.fc2(x)))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class _ConvBlock(torch.nn.Module):\n",
    "    \"\"\"A Convolutional block.\"\"\"\n",
    "\n",
    "    def __init__(self, *units):\n",
    "        \"\"\"Create a block with len(units) - 1 convolutions.\n",
    "        \n",
    "        convolution number i transforms the number of channels from \n",
    "        units[i - 1] to units[i] channels.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.convs = torch.nn.ModuleList([\n",
    "            torch.nn.Conv2d(in_, out, 3, 1, 1)\n",
    "            for in_, out in zip(units[:-1], units[1:])\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Each convolution is followed by a ReLU, then the block is\n",
    "        # concluded by a max pooling.\n",
    "        for c in self.convs:\n",
    "            x = F.relu(c(x))\n",
    "        return F.max_pool2d(x, 2, 2, 0, ceil_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#패스 지정\n",
    "dataset_path = \"/Users/Administrator/Desktop/facebank\"\n",
    "model_weight_save_path = \"/Users/Administrator/Desktop/pytorch/save/vgg\"\n",
    "num_classes = 400\n",
    "\n",
    "batch_size = 25\n",
    "num_workers = 2\n",
    "lr = 1e-2\n",
    "\n",
    "total_epoch = 100\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "\n",
    "\n",
    "# 훈련 및 테스트데이터 로드\n",
    "traindir = os.path.join(dataset_path, '/Users/Administrator/Desktop/facebank/train')\n",
    "testdir = os.path.join(dataset_path, '/Users/Administrator/Desktop/facebank/train')\n",
    "\n",
    "# normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\n",
    "    traindir,\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        #normalize,\n",
    "    ]))\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True,\n",
    "    num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(testdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        #normalize,\n",
    "    ])),\n",
    "    batch_size=batch_size, shuffle=False,\n",
    "    num_workers=num_workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VggFace(num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CEloss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "total_iteration_per_epoch = int(np.ceil(len(train_dataset)/batch_size))\n",
    "\n",
    "for epoch in range(1, total_epoch + 1):\n",
    "    model.train()\n",
    "    for itereation, (input, target) in enumerate(train_loader):\n",
    "        images = input.to(device)\n",
    "        labels = target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = CEloss(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('Epoch [{}/{}], Iteration [{}/{}] Loss: {:.4f}'.format(epoch, total_epoch, itereation+1, total_iteration_per_epoch, loss.item()))\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), model_weight_save_path + 'model.pt', _use_new_zipfile_serialization=False)\n",
    "        #torch.save(model, model_weight_save_path + 'model.pt') #전체 모델저장\n",
    "        #torch.save(model.state_dict(), model_weight_save_path + 'model' + str(epoch) + \".pth\") #모델 객체의 static 저장\n",
    "        #torch.save({'model': model.state_dict(),'optimizer': optimizer.state_dict()}, PATH + 'all.tar') #학습중 진행상황 등 모든 전체 정보저장\n",
    "        \n",
    "        \n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "      correct = 0\n",
    "      total = 0\n",
    "      for input, target in test_loader:\n",
    "          images = input.to(device)\n",
    "          labels = target.to(device)\n",
    "\n",
    "          # Forward pass\n",
    "          outputs = model(images)\n",
    "          _, predicted = torch.max(outputs.data, 1)\n",
    "          total += len(labels)\n",
    "          correct += (predicted == labels).sum().item()\n",
    "\n",
    "      print('Epoch [{}/{}], Test Accuracy of the model on the {} test images: {} %'.format(epoch, total_epoch, total, 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
